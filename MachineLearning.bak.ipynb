{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "<br>\n",
    "\n",
    "## Coursera Machine Learning with Andrew Ng\n",
    "[https://www.coursera.org/learn/machine-learning/home/welcome](https://www.coursera.org/learn/machine-learning/home/welcome)\n",
    "- [Unit 1 - Linear Regression](#unit1)\n",
    "- [Unit 2 - Multivariate Linear Regression](#unit2)\n",
    "- [Unit 3 - Logistic Regression](#unit3)\n",
    "- [Unit 7 - SVM](#unit7)\n",
    "- [Unit 8 - Unsupervised Learning & Dimension Reduction](#unit8)\n",
    "- [Unit 9 - Anomaly Detection](#unit9)\n",
    "___\n",
    "\n",
    "<br>\n",
    "## Python Machine Learning\n",
    "[https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)\n",
    "- Decision Trees and Ensemble Methods\n",
    "- Natural Language Processing (NLP)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera Machine Learning\n",
    "\n",
    "*\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\" - Tom Mitchell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unit1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs unsupervised learning\n",
    "\n",
    "- **Supervised learning:** given an input dataset and corresponding output, train computer to make predictions. Two types:\n",
    " 1. *Regression* - predicts a continuous output\n",
    " 1. *Classification* - predicts a discrete output\n",
    " <br><br>\n",
    "- **Unsupervised learning:** given dataset with no output, derive structure from data, typically done with clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable\n",
    "\n",
    "Univariate linear regression is a type of supervised learning used to predict a single output value $y$ from a single input value $x$, with a given number of examples $m$. The $i^{th}$ training example is represented by $(x^{(i)}, y^{(i)})$.\n",
    "\n",
    "We model the dataset/system with a ***hypothesis function*** $h$ that maps the input variable $x$ to the output variable $y$. For univariate linear regression,\n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$$\n",
    "\n",
    "To calculate the ***parameters*** $\\theta s$ in the hypothesis function, choose values for the parameters so that $h_{\\theta}(x)$ is close to the given actual output values $y$. This can be achieved by minimizing the quantity,\n",
    "\n",
    "$$\\min_{\\theta_{0}, \\ \\theta_{1}} \\ \\frac{1}{2m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}]^{2}$$\n",
    "where,\n",
    "$$ h_{\\theta}(x^{(i)}) = \\theta_{0} + \\theta_{1}x^{(i)} $$\n",
    "\n",
    "The term inside the summation is called the ***squared error cost function*** and is typically written,\n",
    "\n",
    "$$ J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}]^{2} $$\n",
    "\n",
    "So the idea is to use an algorithm that can select values for the parameters that minimize the value of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Algorithm used for function minimization, for univariate linear regression, this method will calculate values for $\\theta_{0}$ and $\\theta_{1}$ that minimize $J$.\n",
    "\n",
    "**Basic procedure:**\n",
    "1. Start with a guess for the value of $\\theta_{0}$ and $\\theta_{1}$.\n",
    "1. Calculate $J$, then select new values of $\\theta_{0}$ and $\\theta_{1}$ until $J$ is minimized.\n",
    "\n",
    "**Definition:**\n",
    "$$ repeat \\ until \\ convergence: \\  \\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta_{0}, \\theta_{1}) \\ \\ for \\ j=0 \\ and \\  j=1  $$\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "Gradient descent uses ***simultaneous update*** where the right hand side of the equation is calculated for all $js$ then all $js$ are updated at the same time before performing the next iteration of gradient descent. The parameter $\\alpha$ is defined as the ***learning rate***.\n",
    "\n",
    "After each iteration, the current value of $\\theta$ moves close to the minimum of $J$ as a function of $\\theta$.\n",
    "\n",
    "Selecting $\\alpha$ is important because if the value is too large, the update at each step may pass over the actual minimum $J$ and fail to find the minimum. If $\\alpha$ is too small, the process will take a long time to complete.\n",
    "\n",
    "When all training examples are used in each step, the process is called ***batch gradient descent***.\n",
    "\n",
    "**For univariate linear regression:**\n",
    "\n",
    "The derivative of the cost function $J$ relative to the parameters $\\theta$ yields ( remember $h_{\\theta}(x^{(i)}) = \\theta_{0} + \\theta_{1}x^{(i)}$ ),\n",
    "$$ \\theta_{0} := \\theta_{0} - \\alpha  \\frac{1}{m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}] $$\n",
    "$$ \\theta_{1} := \\theta_{1} - \\alpha  \\frac{1}{m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}] \\cdot x^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unit2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "Instead of one input variable $x$ there are now $n$ input variables or features, $x_{0}, x_{1}, ..., x_{n}$. Thus the hypothesis function becomes,\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ... + \\theta_{n} x_{n} $$\n",
    "\n",
    "where by convention, $x_{0} = 1$. This expression can be simplified by using column vectors to represent $x$ and $\\theta$ in which case,\n",
    "\n",
    "$$ x = \\begin{pmatrix}x_{0}\\\\x_{1}\\\\.\\\\x_{n}\\end{pmatrix} , \\qquad \\theta = \\begin{pmatrix}\\theta_{0}\\\\\\theta_{1}\\\\.\\\\\\theta_{n}\\end{pmatrix} , \\qquad h_{\\theta}(x) = \\theta^{T} x \\ = (\\theta_{0} \\ \\theta_{1} \\ ... \\ \\theta_{n})\\begin{pmatrix}x_{0}\\\\x_{1}\\\\.\\\\x_{n}\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate gradient descent\n",
    "\n",
    "Instead of two update rules for $\\theta_{0}$ and $\\theta_{1}$ there are now $n$ update rules for each feature which can be written,\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j} - \\alpha  \\frac{1}{m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}] \\cdot x_{j}^{(i)} $$\n",
    "\n",
    "It is import to make sure the features are on a similar scale so that they converge at a similar rate. One means of doing this is to use ***feature scaling***; subtract the mean value then divide by the maximum minum minimum value or the standard deviation of the feature. Ideally, features should be in the range [-1,1].\n",
    "\n",
    "**How to choose learning rate:**\n",
    "\n",
    "First, to ensure gradient descent is working correctly, plot the minimum value of $J(\\theta)$ as a function of the number of iterations of gradient descent. The curve should be decreasing with a slope that flattens out at higher iterations to indicate it is approaching convergence.\n",
    "\n",
    "If the plot does not decrease monotonically, then it is likely that $\\alpha$ is set too large and should be decreased. Typical range of values to try: \n",
    "\n",
    "$$ \\alpha = \\ ..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ... $$\n",
    "\n",
    "If convergence takes a long time, it is likely that $\\alpha$ is set too small and must be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "The hypothesis function becomes,\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_{0} + \\theta_{1} x + \\theta_{2} x^{2} + ... + \\theta_{n} x^{n} $$\n",
    "\n",
    "where $n$ can also take on values between 0 and 1.\n",
    "\n",
    "Here, feature scaling becomes very important because of the power variation between features in the hypothesis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unit3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "In classification problems it often occurs that the output variable $y \\in \\{0,1\\}$, i.e., there is a positive or negative outcome this is called a ***binary classification problem***.\n",
    "\n",
    "If using linear regression to predict either 0 or 1 values it may seem reasonable to select predictions greater than 0.5 equal to 1 and those below equal to 0. However data points that may not be expected to change the prediction mechanism can alter the linear regression of the data and significantly change the output of the hypothesis function.\n",
    "\n",
    "Instead it is better to use a ***sigmoid function*** rather than a linear function to represent the hypothesis,\n",
    "$$ h_{\\theta}(x) = g(\\theta^{T} x) = \\frac{1}{1 + e^{-\\theta^{T} x}} $$\n",
    "\n",
    "<img src=\"sigmoid.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Logistic regression returns a probability that the hypothesis function outputs to 0 or 1 such that,\n",
    "\n",
    "$$ P(y = 0  \\ | \\ x; \\theta) + P(y = 1 \\ | \\ x; \\theta) = 1 $$\n",
    "\n",
    "The reasonable choice for returning an output of 1 is then, $ h_{\\theta}(x) \\ge 0.5 $ or $\\theta^{T} x \\ge 0$, and an output of 0 when $ h_{\\theta}(x) \\lt 0.5 $ or $\\theta^{T} x \\lt 0$.\n",
    "\n",
    "Thus the ***decision boundary***, the line that separates $y=0$ and $y=1$, occurs where $\\theta^{T} x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function for logistic regression\n",
    "\n",
    "The cost function for linear regression (based on the sum of the squared error) cannot be used here as it would be non-convex and not guaranteed to converge to the global minima when using gradient descent.\n",
    "\n",
    "Instead we want a function that behaves appropriately,\n",
    "\n",
    "$$ if \\ h_{\\theta} (x) = 0, \\ predict \\ P(y=1 \\ | \\ x;\\theta) = 0, so \\ Cost \\rightarrow \\infty $$\n",
    "$$ if \\ h_{\\theta} (x) = 1, \\ predict \\ P(y=1 \\ | \\ x;\\theta) = 1, so \\ Cost \\rightarrow 0 $$\n",
    "$$ if \\ h_{\\theta} (x) = 1, \\ predict \\ P(y=0 \\ | \\ x;\\theta) = 0, so \\ Cost \\rightarrow \\infty $$\n",
    "$$ if \\ h_{\\theta} (x) = 0, \\ predict \\ P(y=0 \\ | \\ x;\\theta) = 1, so \\ Cost \\rightarrow 0 $$\n",
    "\n",
    "A good selection then given the sigmoid shape of the hypothesis function is,\n",
    "\n",
    "$$ Cost \\ (h_{\\theta} (x), y) = \\begin{cases} -\\log{h_{\\theta} (x)}  & \\mbox{if } y = 1 \\\\ -\\log{(1 - h_{\\theta} (x))} & \\mbox{if } y = 0 \\end{cases} $$\n",
    "\n",
    "Since $y = 0$ or $1$ always, this multipart definition can be written as a single expression,\n",
    "\n",
    "$$ Cost \\ (h_{\\theta} (x), y) = -y \\log{h_{\\theta} (x)} - (1-y) \\log{(1 - h_{\\theta} (x))} $$\n",
    "\n",
    "And,\n",
    "\n",
    "$$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} \\log{h_{\\theta} (x^{(i)})} + (1-y^{(i)}) \\log{(1 - h_{\\theta} (x^{(i)}))} $$\n",
    "\n",
    "This expression can be derived from maximum likelihood estimation theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for logistic regression\n",
    "\n",
    "To minimize $J(\\theta)$ by calculating the partial derivative gives,\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j} - \\alpha  \\frac{1}{m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}] \\cdot x_{j}^{(i)} $$\n",
    "\n",
    "which is identical to linear regression, the only difference being the form of the hypothesis function $h_{\\theta}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass logistic regression\n",
    "\n",
    "For systems that have more output options than 0 and 1 as in binary classification. It is possible to iteratively apply binary classification using a ***one-vs-all*** procedure,\n",
    "\n",
    "$$ h_{\\theta}^{(i)} (x) = P(y=i \\ | \\ x;\\theta) \\qquad (i=1,2,3,...) $$\n",
    "\n",
    "Each iteration returns the probability that $y=i$. So for each example, the predicted output is,\n",
    "\n",
    "$$ \\max_{i} h_{\\theta}^{(i)} (x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization to address overfitting\n",
    "\n",
    "One method to reduce variance and overfitting is to penalize certain features and make the corresponding parameters very small so that they have little influence in determining the value of the cost function. This can be achieved by adding the product of the square of the features with a large number,\n",
    "\n",
    "$$ \\min_{\\theta} \\frac{1}{2m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}]^{2} + \\lambda \\sum_{j=1}^{n} \\theta_{j}^{2} $$\n",
    "\n",
    "If the ***regularization parameter*** $\\lambda$ is set too high, the solution may end up underfitting.\n",
    "\n",
    "Note that for linear regression, $\\theta_{0}$ is not regularized, i.e., it is not included in the summation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and linear regression\n",
    "\n",
    "The gradient descent formula becomes,\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j} (1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}] \\cdot x_{j}^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and logistic regression\n",
    "\n",
    "The cost function is,\n",
    "\n",
    "$$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)} \\log{h_{\\theta} (x^{(i)})} - (1-y^{(i)}) \\log{(1 - h_{\\theta} (x^{(i)}))}] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_{j}^{2} $$\n",
    "\n",
    "And the gradient descent formula is identical to that for linear regression with the exception that the hypothesis function $h_{\\theta}(x)$ is different,\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j}  - \\alpha \\Big[ \\Big( \\frac{1}{m} \\sum_{i=1}^{m} \\ [h_{\\theta}(x^{(i)}) - y^{(i)}] \\cdot x_{j}^{(i)} \\Big) + \\frac{\\lambda}{m} \\theta_{j} \\Big] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unit4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machines\n",
    "\n",
    "In SVMs, the optimization objective is to maximize the ***margin***. The margin is defined as the distance between the separating hyperplane (decision boundary) and the training samples that are closest to this hyperplane, which are the so-called ***support vectors***. The rationale behind having decision boundaries with large margins is that they tend to have a lower generalization error whereas models with small margins are more prone to overfitting.\n",
    "\n",
    "SVM can be derived by modifying logistic regression. Consider the cost function written as,\n",
    "\n",
    "$$ \\min_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} {cost}_{(y=1)} \\Big( \\theta^{T} x^{(i)} \\Big) + \n",
    "(1-y^{(i)}) {cost}_{(y=0)} \\Big( \\theta^{T} x^{(i)} \\Big) + \\frac{\\lambda}{2m} \\sum_{j=0}^{n} \\theta_{j}^{2} $$\n",
    "\n",
    "Drop the use of $m$ and define regularization as $C\\cdot A + B$ instead of $A + \\lambda \\cdot B$ such that $C = 1/\\lambda$,\n",
    "\n",
    "$$ \\min_{\\theta} C \\sum_{i=1}^{m} y^{(i)} {cost}_{(y=1)} \\Big( \\theta^{T} x^{(i)} \\Big) + \n",
    "(1-y^{(i)}) {cost}_{(y=0)} \\Big( \\theta^{T} x^{(i)} \\Big) + \\frac{1}{2} \\sum_{j=0}^{n} \\theta_{j}^{2} $$\n",
    "\n",
    "The portions of the cost function $cost \\ ( \\theta^{T} x^{(i)} )$ require,\n",
    "$$ If \\ y=1, \\ \\theta^{T} x^{(i)} \\ge 1 $$\n",
    "$$ If \\ y=0, \\ \\theta^{T} x^{(i)} \\le -1 $$\n",
    "\n",
    "To simplify the derivation, assume a large regularization parameter $C$ which simplifies the optimization objective to,\n",
    "\n",
    "$$ \\min_{\\theta} \\frac{1}{2} \\sum_{j=0}^{n} \\theta_{j}^{2} $$\n",
    "\n",
    "The dot product of two vectors can be written as the product of the projection of one vector on to the other times its norm,\n",
    "\n",
    "$$ \\theta^{T} x^{(i)} = p^{(i)} \\cdot \\| \\theta \\| $$\n",
    "\n",
    "Thus minimizing $\\theta$ is equivalent to choosing $\\theta$ that maximizes $p^{(i)}$. This is why SVM is called *large margin optimization* or why SVM selects hyperplanes that maximize the separation between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Create landmarks in feature space and define a similarity function $f$ that relates how similar example $x$ is to each landmark $l$. A function that works well to describe this relationship is the *Gaussian or radial basis function*,\n",
    "\n",
    "$$ f_{i}(x) = \\exp{- \\frac{\\|x - l^{(i)} \\|^{2}}{2\\sigma^{2}}} $$\n",
    "\n",
    "The hypothesis function for *three* features becomes,\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_{0} + \\theta_{1} f_{1} + \\theta_{2} f_{2} + \\theta_{3} f_{3} $$\n",
    "\n",
    "And we predict $y=1$ if,\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta^{T} f = \\theta_{0} + \\theta_{1} f_{1} + \\theta_{2} f_{2} + \\theta_{3} f_{3} \\ge 0 $$\n",
    "\n",
    "How are the landmarks determined? For each example in the training set, set landmarks at the location of the training examples so in the end there are $m$ total examples.\n",
    "\n",
    "Now the cost function becomes,\n",
    "\n",
    "$$ \\min_{\\theta} C \\sum_{i=1}^{m} y^{(i)} {cost}_{(y=1)} \\Big( \\theta^{T} f^{(i)} \\Big) + \n",
    "(1-y^{(i)}) {cost}_{(y=0)} \\Big( \\theta^{T} f^{(i)} \\Big) + \\frac{1}{2} \\sum_{j=1}^{m} \\theta_{j}^{2} $$\n",
    "\n",
    "Remember that there are $m$ features and thus $m$ parameters $\\theta$,\n",
    "\n",
    "$$ \\theta^{T} f = \\theta_{0} + \\theta_{1} f_{1} + \\theta_{2} f_{2} + ... + \\theta_{m} f_{m} $$\n",
    "\n",
    "The value of $\\sigma$ used in the similarity function require tuning. Smaller values increase the influence or reach of the training examples leading to a softer boundary. Larger values will cause the boundary to tightly trace all but 1 of the classes, which can lead to generalization error on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unit8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning: K-means clustering\n",
    "\n",
    "Two step algorithm that is repeated until convergence:\n",
    "1. Cluster assignment\n",
    "1. Centroid calculation\n",
    "\n",
    "Cluster centroids are randomly initialized before the algorithm begins. Cluster assignment of each example is based on Euclidean distance between example and cluster centroids. This is reflected in the cost function of the optimization objective also called the ***inertia***,\n",
    "\n",
    "$$ \\min_{c, \\ \\mu} \\ \\frac{1}{m} \\sum_{i=1}^{m} \\| x^{(i)} - \\mu_{c^{(i)}} \\|^{2} $$\n",
    "\n",
    "where $c$ is the cluster index and $\\mu$ is the location of the cluster centroid.\n",
    "\n",
    "### Initializing cluster centroids:\n",
    "\n",
    "Different choices of initialization can yield different results, the solution of k-means may converge to a local optima. To prevent this from occurring:\n",
    "\n",
    "1. Randomly select centroids to be specific training examples. \n",
    "1. Run k-means (optimization objective) and minimize cost function.\n",
    "1. Repeat steps 1 and 2 (perhaps 100) and select the k-means results that have the lowest cost.\n",
    "\n",
    "### Choosing number of clusters:\n",
    "\n",
    "Most common method is to select number of clusters manually based on some downstream purpose (i.e., market segmentation such as t-shirt sizing).\n",
    "\n",
    "**Elbow method**: Plot the cost function for various number of clusters. Select the solution that corresponds to the location where the plot flattens out. It is possible that this method may not work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction: Principal component analysis\n",
    "\n",
    "It may be possible to reduce the number of features in the training set without sacrificing significant model performance.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Compute covariance matrix from the standardized dataset,\n",
    "$$ \\Sigma = \\frac{1}{m} \\sum_{i=1}{n} (x^{(i)})(x^{(i)})^{T} $$\n",
    "1. Compute eigenvectors of $\\Sigma$ using singular value decomposition,\n",
    "$$ U, \\ S, \\ V = svd(\\Sigma) $$ <br>\n",
    "\n",
    "Here the first $k$ column vectors of $U$ will transform the $n$ dimensional space data to the reduced $k$ dimensional space, $z = U^{T} x.$\n",
    "\n",
    "To recover approximate representations of the original data from the compressed/transformed data, $x_{approx} = U z.$\n",
    "\n",
    "The basis of the transformed datasets are then the eigenvectors of the covariance matrix, which are called *principal components*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unit9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "Assume an unlabeled dataset that contains clean and anomalous data where each example $x^{(i)}$ is $x \\in \\mathbb{R}^{n}$. Then apply multivariate density estimation to each example (assuming features are independent),\n",
    "\n",
    "$$ p(x) = p(x_{1}; \\ \\mu_{1}, \\sigma_{1}^{2}) \\cdot p(x_{2}; \\ \\mu_{2}, \\sigma_{2}^{2}) \\cdot \\ldots \\cdot p(x_{n}; \\ \\mu, \\sigma_{n}^{2}) = \\prod_{j=1}^{n} \\ p(x_{j}; \\mu_{j} \\sigma_{j}^{2}) $$\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "1. Choose features that may be indicative of anomalous examples.\n",
    "1. Fit parameters for all $n$ features $\\mu_{n}$, $\\sigma_{n}^{2}$,\n",
    "$$ \\mu_{j} = \\frac{1}{m} \\sum_{i=1}^{m} x_{j}^{(i)}, \\quad \\sigma_{j}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} \\big(x_{j}^{(i)} - \\mu \\big)^{2} $$\n",
    "1. Given new example $x$, compute multivariate density estimate of all the relevant features of this example $p(x)$,\n",
    "$$ p(x) = \\prod_{j=1}^{n} \\ p(x_{j}; \\mu_{j} \\sigma_{j}^{2}) = \\prod_{j=1}^{n} \\frac{1}{\\sqrt{2 \\pi} \\sigma_{j}} \\exp - \\Big( \\frac{(x_{j} - \\mu_{j})^2}{2 \\sigma_{j}^{2}} \\Big) $$\n",
    "1. Compare $p(x) < \\epsilon$, if true then this example $x$ is an anomaly.\n",
    "\n",
    "<br>\n",
    "### Pseudo-supervised learning approach:\n",
    "\n",
    "Create dataset with labeled anomalous examples ($y=0$ if normal, $y=1$ If anomalous) and divide into CV and test sets.\n",
    "\n",
    "1. Fit parameters for all $n$ features $\\mu_{n}$, $\\sigma_{n}^{2}$ for training set.\n",
    "1. Evaluate $p(x)$ for each training example and predict $y=1$ if $p(x) < \\epsilon$ and $y=0$ if $p(x) \\ge \\epsilon$.\n",
    "1. Use a good unbalanced class metric such as F1-score to evaluate model on CV/test set.\n",
    "1. Can use CV set to select appropriate $\\epsilon$ and features to include in model before running on test set.\n",
    "\n",
    "<br>\n",
    "### Feature selection:\n",
    "\n",
    "1. ***Check if features are Gaussian*** (not strictly necessary) by plotting distribution. If feature has log-normal distribution, replace feature $x$ with $\\log{x}$ or $\\log{x + c}$. If the feature has an exponential distribution, can adjust feature distribution with power: $x^{1/p}$.\n",
    "1. ***Use error analysis*** so that $p(x)$ is small for anomalous examples and large for normal examples. Choose quick algorithm, run it, refine it so that anomalous values that are predicted to be normal are no long misclassified.\n",
    "\n",
    "<br>\n",
    "### Multivariate gaussian distribution:\n",
    "\n",
    "1. Fit model $p(x)$ by calculating mean average $\\mu$ and covariance $\\Sigma$,\n",
    "$$ \\mu_{j} = \\frac{1}{m} \\sum_{i=1}^{m} x_{j}^{(i)}, \\quad \\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} \\big(x^{(i)} - \\mu \\big) \\big(x^{(i)} - \\mu \\big)^{T} $$\n",
    "1. Given a new example $x$, compute,\n",
    "$$ p(x) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} \\ \\exp{\\Big( -\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1} (x-\\mu) \\Big)} $$\n",
    "1. Classify as an anomaly if $p(x) < \\epsilon$.\n",
    "\n",
    "This method does not assume the features are independent (i.e., the axes of the multivariate Gaussian distribution are orthogonal, for 2D this means the Gaussian top view is an ellipse) as was done with the original model. Another way of describing this relationship is to say the original model is the special case where the covariance matrix is diagonal.\n",
    "\n",
    "### Separable model vs multivariate:\n",
    "\n",
    "1. Multivariate will capture correlations between features where separable model would new features to capture these relationships.\n",
    "1. Separable model is computationally cheaper since the matrix inversion $\\Sigma^{-1}$ is not needed.\n",
    "1. If number of examples $m$ is small, separable model can be used where as multivariate approach requires $m>n$ so that covariance matrix $\\Sigma$ is invertible. Also if there are linearly-dependent features then $\\Sigma$ is non-invertible, so remove the feature causing the redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><br>\n",
    "___\n",
    "\n",
    "# Python Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric vs Nonparameteric classifiers\n",
    "\n",
    "Using parametric models, we estimate parameters from the training dataset to learn a function that can classify new data points without requiring the original training dataset anymore. In contrast, nonparametric models can't be characterized by a set of parameters, and the number of parameters grows with the training data.\n",
    "\n",
    "### Parametric\n",
    "- Perceptron\n",
    "- Logistic regression\n",
    "- Linear SVM\n",
    "\n",
    "### Nonparametric\n",
    "- Decision trees/random forests\n",
    "- Kernel SVM\n",
    "- KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Additional reference:<br>\n",
    "[http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain](http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain)\n",
    "\n",
    "Decision tree classifiers make predictions by asking a series of questions. The questions to ask are determined by data features. The idea is to find the feature that best splits the target class into the purest possible children nodes (ie: nodes that don't contain a mix of both classes, rather pure nodes with only one class). This is achieved by splitting the data at each node in order to maximize the ***information gain ratio***,\n",
    "\n",
    "$$ \\max \\ IG(D_{p}, f) = I(D_{p}) - \\sum_{j=1}^{m} \\frac{N_{j}}{N_{p}} I(D_{j}) $$\n",
    "\n",
    "Here $f$ is the feature to perform the split, $D_{p}$ and $D_{j}$ are the dataset of the parent and $j$th child node, $N_{p}$ and $N_{j}$ are the number of examples of the parent and $j$th child node, $I$ is the impurity measure. Impurity is the \"opposite\" of information.\n",
    "\n",
    "Information gain ratio biases the decision tree against considering attributes with a large number of distinct values. So it solves the drawback of information gain—namely, information gain applied to attributes that can take on a large number of distinct values might learn the training set too well.\n",
    "\n",
    "The information gain is simply the difference between the impurity of the parent node and the normalized sum of the child node impurities. The lower the impurity of the child nodes, the larger the information gain.\n",
    "\n",
    "To improve computational performance, scikit-learn uses binary decision trees,\n",
    "\n",
    "$$ IG(D_{p}, f) = I(D_{p}) - \\sum_{j=1}^{m} \\frac{N_{left}}{N_{p}} I(D_{left}) - \\frac{N_{right}}{N_{p}} I(D_{right})$$\n",
    "\n",
    "The common ***impurity measures*** are ***Gini impurity***, ***entropy***, and the ***classification error***.\n",
    "1. Entropy maximizes the mutual information in a tree. If $p(i|t)$ is the proportion of examples that belong to class $i$  for node $t$, entropy is 0 if $p(i=1|t)=1$ or $p(i=0|t)=0$ and entropy is 1 if $p(i=1|t)=0.5$ and $p(i=0|t)=0.5$.\n",
    "1. Gini impurity can be understood as a criterion to minimize the probability of misclassification. Similar to entropy, Gini is maximized if the classes are perfectly mixed.\n",
    "1. In practice, both Gini and entropy produce similar results.\n",
    "1. Classification error is a useful criterion for pruning but not recommended for learning.\n",
    "\n",
    "*Feature scaling is not necessary for building decision trees.*\n",
    "\n",
    "### Prediction probability\n",
    "\n",
    "Probabilities are calculated from a frequency vector that is created for each node at training time. The vector collects the frequency values of each class label computed from the class label distribution at that node. Then the frequencies are normalized so that they sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "Random forests are an *ensemble* of decision trees, *weak learners* that are combined together to form a strong learner.\n",
    "\n",
    "### Algorithm:\n",
    "1. Draw a random ***bootstrap*** sample of $n$ examples.\n",
    "1. Grow a decision tree from the bootstrap sample,\n",
    " 1. Randomly select $d$ features without replacement.\n",
    " 1. Split the node using the $d$ feature that provides according to the optimization objective, i.e., maximizing the information gain ratio.\n",
    "1. Repeat steps 1 and 2 $k$ times.\n",
    "1. Aggregrate the predictions be each of the $k$ decision trees by returning a ***majority vote*** (i.e., the class predicted by the most estimators).\n",
    "\n",
    "Typically, the hyperparameter most in need of tuning is the number of trees to use, $k$. Additionally, the parameters $n$ and $d$ can also be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging - building an ensemble from bootstrap samples\n",
    "\n",
    "Rather than using the same training set and taking a majority vote to predict a class, bagging draws bootstrap examples (random samples with replacement) from the training set. One example is random forests (with the exception that random forests also select a random number of features to fit the individual trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting of weak learners (Adaboost)\n",
    "\n",
    "Here, the ensemble consists of weak learners that have only a slight performance advantage over random guessing such as a decision tree stump. To improve performance of the ensemble, weak learners are applied to misclassified training examples, boosting the performance for examples that are difficult to classify.\n",
    "\n",
    "### Algorithm summary:\n",
    "1. Draw a random subset of training samples $d_{1}$ without replacement from the training set and train a weak learner $c_{1}$.\n",
    "1. Draw a second random subset without replacement $d_{2}$ and add 50% of the misclassified examples from $d_{1}$ to train a new weak learner $c_{2}$.\n",
    "1. Find the training examples $d_{3}$ on which $c_{1}$ and $c_{2}$ disagree to train a new weak learner $c_{3}$.\n",
    "1. Combine the output of $c_{1}$, $c_{2}$, and $c_{3}$ via majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "\n",
    "Gradient tree boosting is a generalization of boosting to apply an arbitrary differentiable loss function. Like other boosting approaches it combines weak learners to form a strong learner in iterative fashion.\n",
    "\n",
    "Typically, the process begins with a weak model that predicts the mean of $y$. At successive stage of boosting $1 \\le m \\le M$, the model $F_m(x)$ is replaced with a better model $F_{m+1}(x) = F_{m}(x) + h(x)$. The estimator $h(x)$ is calculated from the residual,\n",
    "\n",
    "$$ h(x) = y - F_{m}(x) $$\n",
    "\n",
    "Here the objective function is to minimize $h(x)$, i.e., the new weak learner introduced will fit $h(x)$ to $y - F_{m}(x)$. Instead of calculating the gradient of the loss function $L(y,F_{m}(x)-h(x))$ to find the parameters to minimize $h(x)$, we fit a decision tree. So all the parameters of the decision tree must be computed. Typically one tree at a time is introduced.\n",
    "\n",
    "### Algorithm overview\n",
    "1. Fit an initial ensemble model\n",
    "$$ F(x) = \\sum_{m=1}^{M} \\gamma_{m} h_{m}(x) $$ \n",
    "1. In each stage, introduce a weak learner to compensate the shortcomings of existing weak learners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "\n",
    "### Bag-of-words\n",
    "\n",
    "This technique represents text as numerical feature vectors.\n",
    "\n",
    "1. Create a ***vocabulary*** of unique ***tokens*** (such as words) from the documents (examples).\n",
    "1. Construct a ***feature vector*** from each document that contains the counts of how often each token (word) appears in each document (example). These feature vectors tend to be sparse.\n",
    "\n",
    "One examples in scikit-learn is the `CountVectorizer`, each index in the feature vector corresponds to an integer count in the dictionary. These values are called ***raw term frequencies***, $tf(t,d)$ - the number of times a term $t$ occurs in document $d$.\n",
    "\n",
    "Another example is the `HashingVectorizer` which uses a hashing function with a specified number of bins that represent tokens. This approach can result in smaller and easier to compute models due to the use of hashing for token lookup and restricting the total number of tokens. However it is not possible to invert to return importance of individual terms and there is no idf.\n",
    "\n",
    "Typically, words are used as tokens however it also possible to generate ***n-grams***, a contiguous sequence of 1 or more words, letters, or symbols.\n",
    "\n",
    "Term frequency of appearance is often not the most useful information. It is often necessary to downweight frequently occurring terms in the feature vectors with a technique called ***term frequency-inverse document frequency***, \n",
    "\n",
    "$$ tf\\mbox{-}idf(t,d) = tf(t,d) \\times idf(t,d) $$\n",
    "\n",
    "The inverse document frequency is,\n",
    "\n",
    "$$ idf(t,d) = \\log{\\frac{n_d}{1+df(t,d)}} $$\n",
    "\n",
    "Where $n_d$ is the number of documents and $df(t,d)$ is the number of documents $d$ that contain term $t$. Using 1 in the denominator assigns a non-zero value to terms that appear in all examples (documents) and the $\\log$ function ensures that low document frequencies are not given too much weight.\n",
    "\n",
    "The implementation of `TfidfTransformer` in scikit-learn uses $1+n_d$ in the numerator of the idf term and the full tf-idf equation is $ tf\\mbox{-}idf(t,d) = tf(t,d) \\times \\big( idf(t,d) + 1 \\big) $. In addition, each feature vector is normalized by its L2-norm to have length 1.\n",
    "\n",
    "It is possible to apply dimensionality reduction of the sparse matrices generated by vectorization using `TruncatedSVD` which is applied to the training data directly rather than the covariance matrix. When used on term-count or tf-idf data, this technique is known as Latent Semantic Analysis (LSA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Processing documents into tokens\n",
    "\n",
    "One common method to *tokenize* a document is to split it into words based on whitespace characters.\n",
    "\n",
    "***Stemming*** involves breaking down a word into its root form so that similar words can be mapped to the same stem. This is implemented with the Natural Language Toolkit (NLTK) for Python.\n",
    "\n",
    "***Lemmatization*** aims to obtain the canonical (grammatically correct) forms of individual words. However this process is more computationally intense and it has been observed that stemming and lemmatization have little impact on the performance of text classification.\n",
    "\n",
    "***Stop-words*** are very common words that appear frequently in texts but are unlikely to have any useful information, such as *is*, *and*, *has*, etc. These words can be removed with the NLTK and removing them can be useful especially when working with raw or normalized term frequencies rather than tf-idf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
